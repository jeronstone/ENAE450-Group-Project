\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{listings} % For code snippets
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\title{ENAE450 Final Project Report}
\author{Brandon Newman, Sriman Selvakumaran, Adithya Sundar, Joshua Stone}
\date{Due: May 18th, 2024}

\begin{document}

\maketitle

\section{Introduction}

The following report is for our group's final project for ENAE450. The final project required us to navigate a maze using the Turtlebot3 Waffe Pi. \par
We utilized ROS2 and Python (\verb|rospy|) to program the navigation code, which utilzed the turtlebot's lidar sensor and camera. \par
The navigation code was tested using the physical robot (See section 3), as well as simulated Gazebo runs (See section 2). 

\section{Simulation}

\subsection{Methods}

\subsubsection{Introduction}

Our team utilized ROS2, Python, and Gazebo to simulate the navigation for the Turtlebot3 Waffle Pi.

\subsubsection{Navigation Code}

\subsection{Results}

\section{Hardware}

\subsection{Methods}

\subsubsection{Introduction}

Our team used ROS2 and Python to implement the following navigation algorithm on the Turtlebot3 Waffle Pi. The code utilizes the Lidar sensor and the front camera.

\subsubsection{Navigation with Lidar}

To start, our navigation ROS node subscribed to the Lidar sensor data topic (\verb|/scan|), and created a publisher for the \verb|/cmd_vel| to move the turtlebot.

\begin{lstlisting}
    self.scan_subscriber = self.create_subscription(
        LaserScan,'/scan',self.scan_subscriber_handler, 10)
    self.cmd_vel_publisher = self.create_publisher(
        Twist, '/cmd_vel', 10)
\end{lstlisting}

\subsubsection{Aruco Marker Detection}

Additionally, our team used the turtlebot's front facing camera to detect Aruco markers. This was done as part of the bonus points for the project. For Aruco marker detection, our team used the opencv library to analyze the camera frame from the turtlebot. \par

First, we import opencv (\verb|cv2|) and define the aruco detection variables. We also import the \verb|Image| class for the camera subscriber:

\begin{lstlisting}
import cv2
from sensor_msgs.msg import Image

dictionary = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_1000)
parameters = cv2.aruco.DetectorParameters()
detector = cv2.aruco.ArucoDetector(dictionary, parameters)
\end{lstlisting}

Next, we created a subscriber to the \verb|/image_raw| topic to get the camera data from the turtlebot:

\begin{lstlisting}
self.scan_subscriber = self.create_subscription(
    Image,'/image_raw',self.frame_handler,10)
\end{lstlisting}

In the subscriber callback function, we use the open cv library to detect arucos on the frame:

\begin{lstlisting}
def frame_handler(image_data, self):
    # get camera frame data
    frame = image_data.data

    # detect arucos
    (corners, ids, rejected) = detector.detectMarkers(frame)
\end{lstlisting}

TODO explain how we interfaced this with rest of code

\subsection{Results}

\section{Retrospective}

\subsection{What Went Well}

\subsection{What We Would've Done Differently}

\subsection{Who Did What}

\end{document}
